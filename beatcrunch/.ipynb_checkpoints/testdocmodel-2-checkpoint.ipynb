{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "\tdef __init__(self, doc_list, labels_list):\n",
    "\t\tself.labels_list = labels_list\n",
    "\t\tself.doc_list = doc_list\n",
    "\tdef __iter__(self):\n",
    "\t\tfor idx, doc in enumerate(self.doc_list):\n",
    "\t\t\tyield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['là', 'si', 'ça', 'aussi', 'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'leurs', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'où', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'ceci', 'cela', 'celà', 'cet', 'cette', 'ici', 'ils', 'les', 'leurs', 'quel', 'quels', 'quelle', 'quelles', 'sans', 'soi', 'tout', 'toutes', 'toute', 'tous', ',', '\"', ';', ':', '.', '?', '!', '*', '—', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'c’', 'd’', 'j’', 'l’', 'm’', 'n’', 's’', 't’', 'qu’', 'a', 'avoir', 'ayant', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent', 'être', 'été', 'étée', 'étées', 'étés', 'étant', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "# stopwords_french = stopwords.words(\"french\")\n",
    "# print(stopwords_french)\n",
    "#création de la liste de stopwords\n",
    "stopwords_base = ['là','si','ça','aussi','au','aux','avec','ce','ces','dans','de','des','du','elle','en','et','eux','il','je','la','le','leur','leurs','lui','ma','mais','me','même','mes','moi','mon','ne','nos','notre','nous','on','ou','où','par','pas','pour','qu','que','qui','sa','se','ses','son','sur','ta','te','tes','toi','ton','tu','un','une','vos','votre','vous','ceci','cela','celà','cet','cette','ici','ils','les','leurs','quel','quels','quelle','quelles','sans','soi', 'tout', 'toutes', 'toute', 'tous']\n",
    "stopwords_ponctuation = [',','\"',';',':','.','?','!','*','—']\n",
    "stopwords_lettres_seules = ['c','d','j','l','à','m','n','s','t','y',\"c’\",\"d’\",\"j’\",\"l’\",\"m’\",\"n’\",\"s’\",\"t’\",\"qu’\"]\n",
    "stopwords_verbeterne_etre = ['être','été','étée','étées','étés','étant','suis','es','est','sommes','êtes','sont','serai','seras','sera','serons','serez','seront','serais','serait','serions','seriez','seraient','étais','était','étions','étiez','étaient','fus','fut','fûmes','fûtes','furent','sois','soit','soyons','soyez','soient','fusse','fusses','fût','fussions','fussiez','fussent']\n",
    "stopwords_verbeterne_avoir = ['a','avoir','ayant','eu','eue','eues','eus','ai','as','avons','avez','ont','aurai','auras','aura','aurons','aurez','auront','aurais','aurait','aurions','auriez','auraient','avais','avait','avions','aviez','avaient','eut','eûmes','eûtes','eurent','aie','aies','ait','ayons','ayez','aient','eusse','eusses','eût','eussions','eussiez','eussent']\n",
    "\n",
    "#création du stopset\n",
    "stopset = stopwords_base\n",
    "for w in stopwords_ponctuation:\n",
    "    stopset.append(w)\n",
    "\n",
    "for w in stopwords_lettres_seules:\n",
    "    stopset.append(w)\n",
    "\n",
    "for w in stopwords_verbeterne_avoir:\n",
    "    stopset.append(w)\n",
    "\n",
    "for w in stopwords_verbeterne_etre:\n",
    "    stopset.append(w)\n",
    "    \n",
    "print(stopset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions\n",
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_clean(data):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    new_str = data.lower()\n",
    "    dlist = tokenizer.tokenize(new_str)\n",
    "    #stopwords_french_1 = set(line.strip() for line in open('../dictionaries/stopwords_fr1'))\n",
    "    #stopwords_french_1 =  stopwords.words(\"french\") + ['les', 'a', 'à', 'où', 'plus', 'moins']\n",
    "    stopwords_french_1 =  stopset    \n",
    "    cleanList = [word for word in dlist if word not in stopwords_french_1]\n",
    "    #print(cleanList)\n",
    "    return cleanList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadjson(json_file) :\n",
    "    try :\n",
    "        if os.path.exists(json_file) :\n",
    "            with open(json_file) as f:\n",
    "                return json.load(f)\n",
    "        else :\n",
    "            return {}\n",
    "    except :\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset\n",
    "Select only french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(json_files):\n",
    "    docLabels = []\n",
    "    data = []\n",
    "\n",
    "    for jfile in json_files:\n",
    "        j = loadjson(jfile)\n",
    "        for news in j :\n",
    "            if news != \"statistics\" :\n",
    "                #print(news)\n",
    "                for t in j[news] :\n",
    "                    if t['lang'] == 'fr' :\n",
    "                        # Create uniq title label\n",
    "                        docLabels.append(news+\" - \"+t['title'])\n",
    "                        data.append(nlp_clean(t['text']))\n",
    "                        #data.append(t['text'])\n",
    "    return data, docLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(json_files) :\n",
    "    startime = time.time()\n",
    "\n",
    "    data, docLabels = build_dataset(json_files)\n",
    "    \n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=300, window=10, min_count=0, workers=8,alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(10):\n",
    "        print(\"Training epoch {}\".format(epoch))\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        model.alpha -= 0.002 # decrease the learning rate\n",
    "        model.min_alpha = model.alpha # fix the learning rate, no deca\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "\n",
    "    #saving the created model\n",
    "    model.save(os.path.join('/tmp/doc2vec.w2v'))\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING\n",
    "## open JSON for articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this finds our json files\n",
    "path_to_json = 'articles/'\n",
    "json_files = [path_to_json + pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9956 articles loaded for model\n",
      "Training epoch 0\n",
      "Training epoch 1\n",
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# avec http://www.iero.org/beatcrunch/20171025.json\n",
    "#json_files = sorted(glob.glob('20171025.json'))\n",
    "train_model(json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('/tmp/doc2vec.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test with text existing in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1500574901070 - Sega débarque du passé sur les mobiles', 0.9839634895324707), ('1500579404472 - Sega débarque du passé sur les mobiles', 0.983874499797821), ('1500581201923 - Sega débarque du passé sur les mobiles', 0.9825841188430786), ('1500563204267 - Sega débarque du passé sur les mobiles', 0.9823078513145447), ('1500559608655 - Sega débarque du passé sur les mobiles', 0.9822750687599182), ('1500567702617 - Sega débarque du passé sur les mobiles', 0.9822436571121216), ('1500568612144 - Sega débarque du passé sur les mobiles', 0.9821775555610657), ('1500570407277 - Sega débarque du passé sur les mobiles', 0.9821608662605286), ('1500565001898 - Sega débarque du passé sur les mobiles', 0.9820943474769592), ('1500583902187 - Sega débarque du passé sur les mobiles', 0.9820334911346436)]\n"
     ]
    }
   ],
   "source": [
    "sims = d2v_model.docvecs.most_similar(positive=['1500584801776 - Sega débarque du passé sur les mobiles'])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test with text outside training text\n",
    "infer_vector function have to be carefully set, especially if the text is not long.\n",
    "- steps have to be big (default is really not enough)\n",
    "- alpha should close to the one used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devrait afficher 1508966362314 - Misfit lancera finalement sa Vapor le 31 octobre\n",
      "[('1502395811350 - La montre connectée de Fitbit encore en fuite', 0.30221953988075256), (\"1504085722164 - Fitbit fait équipe avec Adidas dans sa course contre l'Apple Watch\", 0.3014877736568451), ('1502953811827 - Hyundai prévoit un véhicule électrique de 500 km après 2021', 0.2901829779148102), (\"1502875211745 - La corde à sauter connectée Smart Rope Pure est sur l'Apple\\xa0Store\", 0.28938812017440796), (\"1503398121591 - La start-up chinoise HiScene lève 12,75 millions d'euros\", 0.27898749709129333), ('1503987315978 - Apple s’occupe du support de Beddit', 0.27834364771842957), ('1500884710927 - Razer prépare un smartphone pour les utilisateurs mobiles les plus joueurs', 0.2773747444152832), ('1504367438943 - Samsung persiste et signe dans les montres connectées', 0.2750348150730133), (\"1501149930414 - Assistant connecté : Xiaomi lance son clone d'Echo... à 40 euros\", 0.2681073248386383), (\"1506368133246 - Fitibit lancera la commercialisation de l'Ionic le 1er octobre\", 0.2674853503704071)]\n"
     ]
    }
   ],
   "source": [
    "newdoc=\"Dévoilée pendant le dernier CES, la nouvelle smartwatch de Misfit embarque un écran AMOLED rond de 1,39 pouces et d'une résolution de 326ppp, ainsi qu'un processeur Snapdragon Wear 2100, un accéléromètre, un altimètre, un gyroscope, un cardiofréquencemètre, un GPS et un micro. Tous ces capteurs permettront à la Vapor de surveiller la plupart des activités sportives sans dépendre d'un smartphone, et même de diffuser dans un casque bluetooth la musique stockée dans sa mémoire de 4Go. La smartwatch se démarquera par ailleurs par sa lunette tactile, laquelle pourra par exemple être utilisée pour naviguer dans les applications en faisant glisser son doigt autour de l'écran et en cliquant sur le bouton latéral. Misfit promet enfin une autonomie d'environ deux jours entre deux charges. La Vapor sera en vente le 31 octobre au prix de 199$ sur le site du constructeur.\"\n",
    "\n",
    "print('Devrait afficher 1508966362314 - Misfit lancera finalement sa Vapor le 31 octobre')\n",
    "new_vector = d2v_model.infer_vector(nlp_clean(newdoc), steps=5000, alpha=0.025)\n",
    "sims = d2v_model.docvecs.most_similar(positive=[new_vector])\n",
    "print(sims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
